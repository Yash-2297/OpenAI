# -*- coding: utf-8 -*-
"""
Created on Mon May 15 22:33:18 2023

@author: yp229
"""

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS
# from langchain.document_loaders import PyPDFLoader

import os                                      
import getpass
from PyPDF2 import PdfReader


os.environ['OPENAI_API_KEY'] = "API KEY
"

doc_reader =  PdfReader(r"D:\TUM\Thesis\Master Thesis\Literature\MKBHD - Google Docs.pdf")


# read data from the file and put them into a variable called raw_text
raw_text = ''
for page_number in range(len(doc_reader.pages)):
        page = doc_reader.pages[page_number]
        content = page.extract_text()
        if content:
            raw_text += content
            

# Splitting up the text into smaller chunks for indexing
text_splitter = CharacterTextSplitter(        
    separator = "\n",
    chunk_size = 1000,
    chunk_overlap  = 200, #striding over the text
    length_function = len,
)
texts = text_splitter.split_text(raw_text)

## Making the embeddings 

embeddings = OpenAIEmbeddings()
docsearch = FAISS.from_texts(texts, embeddings)

docsearch.embedding_function

# query = "how does GPT-4 change social media?"
# docs = docsearch.similarity_search(query)

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

chain = load_qa_chain(OpenAI(), chain_type="stuff") # we are going to stuff all the docs in at once

# check the prompt
chain.llm_chain.prompt.template

query = "who are the authors of the book?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)




from langchain.chains import RetrievalQA

# set up FAISS as a generic retriever 
retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k":4})

# create the chain to answer questions 
rqa = RetrievalQA.from_chain_type(llm=OpenAI(), 
                                  chain_type="stuff", 
                                  retriever=retriever, 
                                  return_source_documents=True)

query = "What is step by step apporach for Audi as per this paper?"
rqa(query)['result']

query = "Please answer the question based on the document:What are the advantages and disadvantages of implementing big data analytics?"
rqa(query)['result']


query = "Please answer the question based on the document: What are the obstacles AUDI may encounter for big data analytics?"
rqa(query)['result']

query = "Who is Mahatama Gandhi?"
rqa(query)['result']


query = "Please answer the question based on the document: Five important takeaways "
rqa(query)['result']

query = "Please answer the question based on the document: Give me more takeaways "
rqa(query)['result']


query = "Please elaborate on first point of previous answer "
rqa(query)['result']
query = "Please elaborate on second point of that answer "
rqa(query)['result']

query = "Give me answer like Shashi Tharoor: what is the best thing about this phone?"
print(rqa(query)['result'])


from youtube_transcript_api import YouTubeTranscriptApi

YouTubeTranscriptApi.get_transcript("MqyXD-KsaTk")
# iterate over all available transcripts
for transcript in transcript_list:

    # the Transcript object provides metadata properties
    print(
        transcript.video_id,
        transcript.language,
        transcript.language_code,
        # whether it has been manually created or generated by YouTube
        transcript.is_generated,
        # whether this transcript can be translated or not
        transcript.is_translatable,
        # a list of languages the transcript can be translated to
        transcript.translation_languages,
    )

    # fetch the actual transcript data
    print(transcript.fetch())

    # translating the transcript will return another transcript object
    print(transcript.translate('en').fetch())
